{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1773824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports for file system operations, regex, JSON logging, text normalization,\n",
    "# dataframe manipulation, numeric utilities, type hints, and file path handling.\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc5601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all raw datasets to attempt loading.\n",
    "# The script automatically scans these, extracts text + label columns, standardizes them,\n",
    "# deduplicates, merges, and optionally splits into train/val/test.\n",
    "INPUT_FILES = [\n",
    "    \"500_anonymized_Reddit_users_posts_labels.csv\",\n",
    "    \"Suicide_Detection.csv\",\n",
    "    \"test.csv\",\n",
    "    \"testset.csv\",\n",
    "    \"Train_suicide1.csv\",\n",
    "    \"twitter-suicidal_data.csv\"\n",
    "]\n",
    "\n",
    "DO_SPLITS = True\n",
    "\n",
    "OUTPUT_COMBINED = \"combined_suicide_ideation_dataset.csv\"\n",
    "TRAIN_OUT = \"combined_train.csv\"\n",
    "VAL_OUT   = \"combined_val.csv\"\n",
    "TEST_OUT  = \"combined_test.csv\"\n",
    "\n",
    "# Ensures reproducible train/test split order\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c374d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known patterns for identifying \"text\" columns in different datasets.\n",
    "TEXT_NAME_HINTS = [\n",
    "    \"text\",\"post\",\"body\",\"content\",\"comment\",\"tweet\",\"message\",\"clean_text\",\"sentence\",\"statement\",\n",
    "    \"post_text\",\"postbody\",\"title\",\"headline\",\"selftext\",\"status\"\n",
    "]\n",
    "\n",
    "# Known indicators for label column names across datasets.\n",
    "LABEL_NAME_HINTS = [\n",
    "    \"label\",\"class\",\"target\",\"is_suicide\",\"is_suicidal\",\"suicide\",\"suicidal\",\"ideation\",\"is_ideation\",\n",
    "    \"category\",\"y\",\"output\",\"sentiment\",\"tag\"\n",
    "]\n",
    "\n",
    "# Tokens that strongly indicate a NON-suicidal label (e.g., control/neutral samples).\n",
    "NON_IDEATION_TOKENS = [\n",
    "    \"non-suicidal\",\"nonsuicidal\",\"non suicidal\",\"non_suicidal\",\"no suicide\",\"control\",\"neutral\",\"others\",\"other\"\n",
    "]\n",
    "\n",
    "# Noise & filler patterns to remove from text (common in web-scraped data)\n",
    "filler_patterns = [\n",
    "    r'\\bfiller\\b', \n",
    "    r'www\\s*youtube\\s*com',\n",
    "    r'www\\s*reddit\\s*com',\n",
    "    r'https?\\s*www\\s*reddit',\n",
    "    r'https?\\s*www\\s*youtube',\n",
    "    r'amp\\s*x200b\\s*amp',\n",
    "    r'x200b\\s*amp\\s*x200b',\n",
    "    r'\\bcake\\b'\n",
    "]\n",
    "\n",
    "filler_regex = re.compile('|'.join(filler_patterns), flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef875b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_read_csv(path: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Try multiple strategies to load a CSV cleanly.\n",
    "    Handles weird encodings, separators, and bad lines.\n",
    "    Returns a DataFrame or None if all attempts fail.\n",
    "    \"\"\"\n",
    "    attempts = [\n",
    "        dict(sep=None, engine=\"python\", encoding=\"utf-8\", on_bad_lines=\"skip\"),\n",
    "        dict(sep=None, engine=\"python\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\"),\n",
    "        dict(sep=None, engine=\"python\", encoding=\"latin1\", on_bad_lines=\"skip\"),\n",
    "        dict(sep=\",\", encoding=\"utf-8\", on_bad_lines=\"skip\"),\n",
    "        dict(sep=\",\", encoding=\"latin1\", on_bad_lines=\"skip\"),\n",
    "        dict(sep=\"\\t\", encoding=\"utf-8\", on_bad_lines=\"skip\"),\n",
    "        dict(sep=\";\", encoding=\"utf-8\", on_bad_lines=\"skip\"),\n",
    "    ]\n",
    "    last_err = None\n",
    "    for kw in attempts:\n",
    "        try:\n",
    "            df = pd.read_csv(path, **kw)\n",
    "            df.columns = [str(c).strip() for c in df.columns]\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    print(f\"[WARN] Could not read {path}: {repr(last_err)}\")\n",
    "    return None\n",
    "\n",
    "def pick_text_columns(df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Heuristically pick which columns contain raw text.\n",
    "    Looks at:\n",
    "        - string dtype\n",
    "        - name similarity to known text column hints\n",
    "        - average length of text values\n",
    "    Returns a list of likely text columns.\n",
    "    \"\"\"\n",
    "    obj_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "    scored = []\n",
    "    for c in obj_cols:\n",
    "        name = c.lower()\n",
    "        name_score = sum(1 for h in TEXT_NAME_HINTS if h in name)\n",
    "        try:\n",
    "            avg_len = df[c].dropna().astype(str).map(len).mean()\n",
    "        except Exception:\n",
    "            avg_len = 0\n",
    "        scored.append((c, name_score, avg_len))\n",
    "    scored.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "    picked = []\n",
    "    if scored:\n",
    "        picked.append(scored[0][0])\n",
    "        # Optionally add a second (e.g., \"title\") if present\n",
    "        for c, ns, al in scored[1:]:\n",
    "            if (\"title\" in c.lower() or \"headline\" in c.lower()) and c not in picked:\n",
    "                picked.append(c); break\n",
    "    return picked\n",
    "\n",
    "def is_binary_like(series: pd.Series) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a series looks like a binary label (~0/1/true/false/etc.).\n",
    "    \"\"\"\n",
    "    vals = pd.Series(series.dropna().unique()).astype(str).str.lower().tolist()\n",
    "    if len(vals) <= 1:\n",
    "        return False\n",
    "    allowed = {\"0\",\"1\",\"true\",\"false\",\"yes\",\"no\",\"y\",\"n\",\"t\",\"f\"}\n",
    "    return all((v in allowed) for v in vals) or (len(vals) == 2)\n",
    "\n",
    "def pick_label_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Try to identify the label column using:\n",
    "      - name similarity to label hints\n",
    "      - small number of unique values\n",
    "      - excluding known text columns\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    for c in df.columns:\n",
    "        lname = c.lower()\n",
    "        name_score = sum(1 for h in LABEL_NAME_HINTS if h in lname)\n",
    "        nunique = df[c].dropna().nunique()\n",
    "        if nunique <= 10 or name_score > 0:\n",
    "            candidates.append((c, name_score, nunique))\n",
    "    candidates.sort(key=lambda x: (x[1], -x[2]), reverse=True)\n",
    "    text_cols = set(pick_text_columns(df))\n",
    "    for c, ns, nu in candidates:\n",
    "        if c not in text_cols:\n",
    "            return c\n",
    "    return candidates[0][0] if candidates else None\n",
    "\n",
    "def to_binary_label(value: object) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Convert a raw label (string/number/category) into a binary 0/1 value.\n",
    "    Returns:\n",
    "        1 = suicidal/ideation/attempt\n",
    "        0 = non-suicidal/control/neutral\n",
    "        None = unknown/unusable\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    s = str(value).strip().lower()\n",
    "    if s in {\"1\",\"true\",\"t\",\"yes\",\"y\"}: return 1\n",
    "    if s in {\"0\",\"false\",\"f\",\"no\",\"n\"}:  return 0\n",
    "    if any(tok in s for tok in NON_IDEATION_TOKENS): return 0\n",
    "    if any(k in s for k in [\"suicid\",\"ideat\",\"attempt\"]): return 1\n",
    "    if s in {\"control\",\"neutral\",\"others\",\"other\",\"none\"}: return 0\n",
    "    if s in {\"positive\",\"negative\",\"pos\",\"neg\"}: return None\n",
    "    try:\n",
    "        fv = float(s)\n",
    "        if fv == 1.0: return 1\n",
    "        if fv == 0.0: return 0\n",
    "        if fv == -1.0: return 0\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def standardize_dataset(df: pd.DataFrame, source_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a raw dataset into a standardized format:\n",
    "        { text, label, source }\n",
    "    - Picks text columns\n",
    "    - Picks label column\n",
    "    - Maps labels to binary\n",
    "    - Drops unusable rows\n",
    "    \"\"\"\n",
    "    tcols = pick_text_columns(df)\n",
    "    if not tcols: return pd.DataFrame(columns=[\"text\",\"label\",\"source\"])\n",
    "    text = df[tcols[0]].astype(str)\n",
    "    if len(tcols) >= 2:\n",
    "        text = (df[tcols[0]].astype(str).fillna(\"\") + \" \" +\n",
    "                df[tcols[1]].astype(str).fillna(\"\"))\n",
    "    lcol = pick_label_column(df)\n",
    "    if lcol is None: return pd.DataFrame(columns=[\"text\",\"label\",\"source\"])\n",
    "    mapped = df[lcol].apply(to_binary_label)\n",
    "\n",
    "    # Fallback mapping if too many Nones\n",
    "    if mapped.notna().mean() < 0.5:\n",
    "        uniq = set(str(x).strip().lower() for x in df[lcol].dropna().unique().tolist())\n",
    "        possible_pos = {\"suicide\",\"suicidal\",\"ideation\",\"attempt\",\"si\",\"s\"}\n",
    "        possible_neg = {\"non-suicide\",\"nonsuicide\",\"non suicidal\",\"control\",\"neutral\",\"others\",\"other\",\"no si\",\"no_si\",\"ns\"}\n",
    "        mapping = {}\n",
    "        for u in uniq:\n",
    "            if u in possible_pos or any(k in u for k in [\"suicid\",\"ideat\",\"attempt\"]):\n",
    "                mapping[u] = 1\n",
    "            elif u in possible_neg or any(k in u for k in [\"non-suicid\",\"non suicidal\",\"nonsuicid\",\"control\",\"neutral\",\"others\"]):\n",
    "                mapping[u] = 0\n",
    "        if mapping:\n",
    "            mapped2 = df[lcol].astype(str).str.strip().str.lower().map(mapping)\n",
    "            mapped = mapped.where(mapped.notna(), mapped2)\n",
    "\n",
    "    out = pd.DataFrame({\"text\": text.astype(str).str.strip(), \"label\": mapped})\n",
    "    out = out.dropna(subset=[\"text\",\"label\"])\n",
    "    out = out[out[\"text\"].str.len() > 0]\n",
    "    out[\"label\"] = out[\"label\"].astype(int)\n",
    "    out[\"source\"] = os.path.basename(source_name)\n",
    "    return out\n",
    "\n",
    "def remove_fillers(text):\n",
    "    \"\"\"\n",
    "    Remove filler/noise using filler_regex and normalize whitespace.\n",
    "    \"\"\"\n",
    "    text = filler_regex.sub(' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7d6f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] No rows after standardizing: test.csv\n",
      "{\n",
      "  \"combined_rows\": 248437,\n",
      "  \"dropped_duplicates\": 717,\n",
      "  \"label_counts\": {\n",
      "    \"1\": 124585,\n",
      "    \"0\": 123852\n",
      "  },\n",
      "  \"output\": \"combined_suicide_ideation_dataset.csv\",\n",
      "  \"sources\": [\n",
      "    \"Suicide_Detection.csv\",\n",
      "    \"Train_suicide1.csv\",\n",
      "    \"testset.csv\",\n",
      "    \"twitter-suicidal_data.csv\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"splits\": {\n",
      "    \"train\": {\n",
      "      \"rows\": 198749,\n",
      "      \"dist\": {\n",
      "        \"1\": 0.5015,\n",
      "        \"0\": 0.4985\n",
      "      },\n",
      "      \"path\": \"combined_train.csv\"\n",
      "    },\n",
      "    \"val\": {\n",
      "      \"rows\": 24844,\n",
      "      \"dist\": {\n",
      "        \"1\": 0.5015,\n",
      "        \"0\": 0.4985\n",
      "      },\n",
      "      \"path\": \"combined_val.csv\"\n",
      "    },\n",
      "    \"test\": {\n",
      "      \"rows\": 24844,\n",
      "      \"dist\": {\n",
      "        \"1\": 0.5014,\n",
      "        \"0\": 0.4986\n",
      "      },\n",
      "      \"path\": \"combined_test.csv\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "Saved to: C:\\Users\\colin\\OneDrive\\Desktop\\DS785_Project\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Check that input files actually exist\n",
    "    existing = [p for p in INPUT_FILES if os.path.exists(p)]\n",
    "    if not existing:\n",
    "        raise SystemExit(\"No input files found. Check INPUT_FILES paths.\")\n",
    "\n",
    "    frames = []\n",
    "    # Loop through input files, standardize them, and collect clean rows\n",
    "    for path in existing:\n",
    "        df = robust_read_csv(path)\n",
    "        if df is None or df.empty:\n",
    "            print(f\"[SKIP] Empty or unreadable: {path}\")\n",
    "            continue\n",
    "        std = standardize_dataset(df, path)\n",
    "        if std.empty:\n",
    "            print(f\"[SKIP] No rows after standardizing: {path}\")\n",
    "            continue\n",
    "        frames.append(std)\n",
    "    \n",
    "    # Require at least one usable dataset\n",
    "    if not frames:\n",
    "        raise SystemExit(\"No standardized datasets produced; nothing to combine.\")\n",
    "\n",
    "    combined = pd.concat(frames, ignore_index=True)\n",
    "    combined[\"text\"] = combined[\"text\"].astype(str).apply(remove_fillers)\n",
    "    before = len(combined)\n",
    "    combined = combined.drop_duplicates(subset=[\"text\"])\n",
    "    dropped = before - len(combined)\n",
    "    \n",
    "    # Print summary JSON to console\n",
    "    combined.to_csv(OUTPUT_COMBINED, index=False, encoding=\"utf-8\")\n",
    "    print(json.dumps({\n",
    "        \"combined_rows\": int(len(combined)),\n",
    "        \"dropped_duplicates\": int(dropped),\n",
    "        \"label_counts\": combined[\"label\"].value_counts().to_dict(),\n",
    "        \"output\": OUTPUT_COMBINED,\n",
    "        \"sources\": sorted(combined[\"source\"].unique().tolist()),\n",
    "    }, indent=2))\n",
    "\n",
    "    if DO_SPLITS:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        train_df, temp_df = train_test_split(\n",
    "            combined, test_size=0.20, stratify=combined[\"label\"], random_state=RANDOM_SEED\n",
    "        )\n",
    "        val_df, test_df = train_test_split(\n",
    "            temp_df, test_size=0.50, stratify=temp_df[\"label\"], random_state=RANDOM_SEED\n",
    "        )\n",
    "        train_df.to_csv(TRAIN_OUT, index=False, encoding=\"utf-8\")\n",
    "        val_df.to_csv(VAL_OUT, index=False, encoding=\"utf-8\")\n",
    "        test_df.to_csv(TEST_OUT, index=False, encoding=\"utf-8\")\n",
    "        \n",
    "        # Print split statistics\n",
    "        print(json.dumps({\n",
    "            \"splits\": {\n",
    "                \"train\": {\"rows\": len(train_df), \"dist\": train_df[\"label\"].value_counts(normalize=True).round(4).to_dict(), \"path\": TRAIN_OUT},\n",
    "                \"val\":   {\"rows\": len(val_df),   \"dist\": val_df[\"label\"].value_counts(normalize=True).round(4).to_dict(),   \"path\": VAL_OUT},\n",
    "                \"test\":  {\"rows\": len(test_df),  \"dist\": test_df[\"label\"].value_counts(normalize=True).round(4).to_dict(),  \"path\": TEST_OUT},\n",
    "            }\n",
    "        }, indent=2))\n",
    "        \n",
    "    # Determine project directory (works in notebook or .py execution)\n",
    "    BASE_DIR = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
    "    BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save all final artifacts to project directory\n",
    "    (combined.to_csv(BASE_DIR / \"combined_suicide_ideation_dataset.csv\", index=False, encoding=\"utf-8\"))\n",
    "    train_df.to_csv(BASE_DIR / \"combined_train.csv\", index=False, encoding=\"utf-8\")\n",
    "    val_df.to_csv(BASE_DIR / \"combined_val.csv\", index=False, encoding=\"utf-8\")\n",
    "    test_df.to_csv(BASE_DIR / \"combined_test.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(\"Saved to:\", BASE_DIR.resolve())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
